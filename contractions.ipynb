{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contractions and Ambiguity\n",
    "## Grammar Based Disambiguation of English Apostrophe+S Contractions in Movie Scripts\n",
    "##### Max Harder, 2919411, max.harder@uni-bielefeld.de\n",
    "A mini project (10 h) for _230028 Natural language processing:\\\\Introduction to text analysis (S) (SoSe 2019)_, Mr Nikolai Ilinykh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treetaggerwrapper: \"FutureWarning: Possible nested set at position […]\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper\n",
    "import nltk\n",
    "import re\n",
    "import glob\n",
    "import spacy # POS tagging, NER\n",
    "import numpy as np # scientific computing\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from pprint import pprint # vgl. https://docs.python.org/3/library/pprint.html\n",
    "from nltk import sent_tokenize\n",
    "from collections import Counter # dict subclass for counting hashable objects\n",
    "from tqdm import tqdm # progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data/'\n",
    "output_path = './output/'\n",
    "file_path = glob.glob(base_path + 'quentin-tarantino_*.txt')\n",
    "file_path_PREPROC = glob.glob(base_path + 'PREPROC_*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Preprocessed File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractions_in_paragraph(this_par_list, all_par_list):\n",
    "    \"\"\"\n",
    "    extract preprocessed sentences that cointain contraction(s)\n",
    "    param1: list of strings of current paragraph\n",
    "    param2: list of all paragraphs\n",
    "    output: appended list of all paragraphs\n",
    "    \"\"\"\n",
    "    joined_par = \" \".join(this_par_list)\n",
    "    # lower and normalize apostrophes\n",
    "    joined_par = re.sub(r\"[“”]\", \"\\\"\", joined_par.lower().replace(\"’\", \"\\'\"))\n",
    "    joined_par = re.sub(r\"[\\(\\)\\[\\]\\{\\}\\*]\", \"\", joined_par)\n",
    "    if re.search(\"\\'s\", joined_par):\n",
    "        t_sents = sent_tokenize(joined_par)\n",
    "        # extract sentence that contains contraction(s)\n",
    "        for sent in t_sents:\n",
    "            if re.search(\"\\'s\", sent):\n",
    "                all_par_list.append(sent)\n",
    "    return all_par_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preproc_file(path_to_files, preproc_file_name):\n",
    "    \"\"\"\n",
    "    check each paragraph for contractions and make file with contraction sentences.\n",
    "    param1: path to files\n",
    "    param2: name of preproc file\n",
    "    output: None; makes file    \n",
    "    \"\"\"\n",
    "    # make empty file\n",
    "    with open(preproc_file_name, 'w', encoding='utf-8') as f:\n",
    "            pass\n",
    "    # iterate files\n",
    "    for file in tqdm(path_to_files):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "        this_par = []\n",
    "        all_par = []\n",
    "        for i, line in enumerate(data):\n",
    "            # avoid indexError when last line is reached\n",
    "            try: \n",
    "                # remove speakers and instructions\n",
    "                if re.match(r\"[A-Z0-9\\.\\-\\(\\) ]+$\", line):\n",
    "                    pass\n",
    "                # continuation in next line\n",
    "                elif re.match(r\".\" , line) and re.match(r\".\" , data[i+1]):\n",
    "                    this_par.append(line.replace(\"\\n\", \"\").strip())\n",
    "                # next line empty\n",
    "                elif re.match(r\".\" , line) and not re.match(r\".\" , data[i+1]):\n",
    "                    this_par.append(line.replace(\"\\n\", \"\").strip())\n",
    "                # empty line\n",
    "                else:\n",
    "                    all_par = contractions_in_paragraph(this_par, all_par)\n",
    "                    this_par = []\n",
    "            # last line belongs to paragraph (cannot check next line)\n",
    "            except IndexError:\n",
    "                this_par.append(line.replace(\"\\n\", \"\").strip())\n",
    "                all_par = contractions_in_paragraph(this_par, all_par)\n",
    "        # join all paragraphs\n",
    "        joined_all_par = \"\\n\".join(all_par)\n",
    "        # append to preproc file\n",
    "        with open(preproc_file_name, 'a', encoding='utf-8') as f:\n",
    "            # write file tag\n",
    "            f.write(\"<meta file=\"+file[7:]+\">\\n\")\n",
    "            # write contraction sentence\n",
    "            f.write(joined_all_par)\n",
    "            f.write(\"\\n\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.04it/s]\n"
     ]
    }
   ],
   "source": [
    "make_preproc_file(file_path, \"./data/PREPROC_scripts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read Preprocessed File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preproc_file(file_path):\n",
    "    \"\"\"\n",
    "    read preprocessed file and make list with data\n",
    "    param1: file path to file with preprocessed contraction sentences\n",
    "    output: list with contraction sentences of each file in sub-lists\n",
    "    \"\"\"\n",
    "    this_file = []\n",
    "    all_files = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not re.match(\"^<meta file=[\\w_-]+\\.txt>$\", line):\n",
    "                this_file.append(line)\n",
    "            else:\n",
    "                all_files.append(this_file)\n",
    "                this_file = []\n",
    "        all_files.append(this_file)\n",
    "        # delete first empty element\n",
    "        del all_files[0]\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length is number of preprocessed files\n",
    "all_files = read_preproc_file(\"./data/PREPROC_scripts.txt\")\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 483),\n",
       " (1, 388),\n",
       " (2, 335),\n",
       " (3, 486),\n",
       " (4, 527),\n",
       " (5, 511),\n",
       " (6, 362),\n",
       " (7, 702),\n",
       " (8, 582),\n",
       " (9, 495)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of sentences with contractions for each file\n",
    "[(index, len(element)) for index, element in enumerate(all_files)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part-of-Speech-Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used tag set: http://www.laurenceanthony.net/software/tagant/resources/treetagger_tagset.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tags(all_files_list):\n",
    "    \"\"\"\n",
    "    POS tag all contraction sentences of each file\n",
    "    param1: list with contraction sentences of each file in sub-lists\n",
    "    output: list of all tagged files in sub-lists    \n",
    "    \"\"\"\n",
    "    all_tagged_files = []\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "    for each_file in tqdm(all_files_list):\n",
    "        tagged_sentences = []\n",
    "        #each_file = each_file[:10] # SUBSET; uncomment for development\n",
    "        for sent in each_file:\n",
    "            tags = tagger.tag_text(sent)\n",
    "            made_tags = treetaggerwrapper.make_tags(tags)\n",
    "            tagged_sentences.append(made_tags)\n",
    "        all_tagged_files.append(tagged_sentences)\n",
    "    return all_tagged_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tagged_files = make_tags(all_files)\n",
    "#all_tagged_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(all_tagged_files[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phrase Chunking (Noun Phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun_chunks(sentence):\n",
    "    '''\n",
    "    extract noun chunks of sentence\n",
    "    param1: sentence (string)\n",
    "    output: list of tuples with start position and noun chunk\n",
    "    '''\n",
    "    nps = [np.text for np in nlp(sentence).noun_chunks]\n",
    "    # get start positions of each noun chunk\n",
    "    # workaround: empty dummy list if same noun chunk occurrs more than once\n",
    "    start = [re.search(str(element), sentence).start() for element in nps if len(re.findall(str(element), sentence)) == 1]\n",
    "    i_nps = zip(start, nps)\n",
    "    return list(i_nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "#extract_noun_chunks(\"this a is a huge car and that is a super huge car.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities(sentence):\n",
    "    \"\"\"extract named entities of sentence into list\"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    nes = [ent.text for ent in doc.ents]\n",
    "    return nes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def classify_contractions(t_sentence):\n",
    "    \"\"\"\n",
    "    takes tagged sentence with contraction and classifies contraction(s)\n",
    "    param1: tagged sentence with contraction (list)\n",
    "    output: type of contraction\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    sent = \" \".join([tag[0] for tag in t_sentence])  # make sentence\n",
    "    all_conts = []\n",
    "    enum_noun_chunks = extract_noun_chunks(sent)  # enumerated\n",
    "    # iterate words in sentence\n",
    "    for i, t_word in enumerate(t_sentence):\n",
    "        # check for each contraction\n",
    "        if re.search(r\"^\\'s$\", t_word[0]) and i != len(t_sentence)-1:\n",
    "\n",
    "            # 1. the possesive marker\n",
    "            # in the beginning of a noun phrase in the article position\n",
    "            if (re.search(r\"N(N|P)(S)?\", t_sentence[i-1][1]) and\n",
    "                (re.search(r\"N(N|P)(S)?\", t_sentence[i+1][1]) or\n",
    "                 re.search(r\"JJ(R|S)?\", t_sentence[i+1][1]) or\n",
    "                 re.search(r\"SENT|,|:\", t_sentence[i+1][1]) or\n",
    "                 re.search(r\"CD\", t_sentence[i+1][1]))):\n",
    "                result = (\"'s\", \"POS\")\n",
    "            # does not use noun chunks (sometimes removes ’s)\n",
    "\n",
    "            # 2. an abbreviation for the copula 'is' (or 'was')\n",
    "            # distinction between present tense and past tense ignored\n",
    "            # in front of an adjective or adverb\n",
    "            elif (re.search(r\"JJ(R|S)?|RB|DT\", t_sentence[i+1][1]) and not   # DT might conflict with 'has'\n",
    "                  re.search(r\"not\", t_sentence[i+1][0])):\n",
    "                result = (\"is/was\", \"VBZ (copula)\")\n",
    "            elif (len(t_sentence)-i > 2 and\n",
    "                  re.search(r\"not\", t_sentence[i+1][0]) and\n",
    "                  re.search(r\"JJ(R|S)?|RB|DT\", t_sentence[i+2][1])):  # DT might conflict with 'has'\n",
    "                result = (\"is/was\", \"VBZ (copula)\")\n",
    "            # in front of a noun phrase\n",
    "            if result == None:  # dummy elif\n",
    "                for noun_chunk in enum_noun_chunks:\n",
    "                    k = re.compile(r'\\b{}\\b'.format(t_sentence[i+1][0]), re.I)\n",
    "                    if re.search(k, noun_chunk[1]) and re.search(t_word[0], sent).start() < noun_chunk[0]:\n",
    "                        result = (\"is/was\", \"VBZ (copula)\")\n",
    "\n",
    "            # 3. an abbreviation for the auxiliary 'is' (or 'was')\n",
    "            # distinction between present tense and past tense ignored                  \n",
    "            # following verb in present participle form\n",
    "            elif re.search(r\"VVG\", t_sentence[i+1][1]):\n",
    "                 result = (\"is/was\", \"VBZ (auxiliary)\")\n",
    "            # perhaps a ‘not’ or an adverb intervening.\n",
    "            elif (len(t_sentence)-i > 2 and\n",
    "                  re.search(r\"RB\", t_sentence[i+1][1]) and\n",
    "                  re.search(r\"VVG\", t_sentence[i+2][1])):\n",
    "                result = (\"is/was\", \"VBZ (auxiliary)\")\n",
    "\n",
    "            # 4. an abbreviation for the auxiliary 'has'\n",
    "            # in front of a past participle verb form\n",
    "            elif re.search(r\"V(B|H|V)N\", t_sentence[i+1][1]):\n",
    "                  result = (\"has\", \"VHZ\")\n",
    "            # perhaps a ‘not’ or an adverb intervening.\n",
    "            elif (len(t_sentence)-i > 2 and\n",
    "                  re.search(r\"RB\", t_sentence[i+1][1]) and\n",
    "                  re.search(r\"V(B|H|V)N\", t_sentence[i+2][1])):\n",
    "                result = (\"has\", \"VHZ\")\n",
    "\n",
    "            # 5. an abbreviation for the auxiliary 'does'\n",
    "            # question and a verb that is neither present participle nor past participle\n",
    "            elif (len(t_sentence)-i > 2 and\n",
    "                  t_sentence[-1][0] == \"?\" and\n",
    "                  re.search(r\"PP\", t_sentence[i+1][1]) and\n",
    "                  (re.search(r\"^V(B|D|H|V)$\", t_sentence[i+2][1]) or\n",
    "                   re.search(r\"got\", t_sentence[i+2][0]))):\n",
    "                result = (\"does\", \"VDZ\")\n",
    "                    \n",
    "            # 6. an abbreviation for the pronoun 'us'\n",
    "            elif t_sentence[i-1][0] == \"let\":\n",
    "                result = (\"us\", \"PP (us)\")\n",
    "                #print(result, t_sentence) # now catches something\n",
    "\n",
    "            # 7. the plural marker for abbreviations, acronyms and numbers.\n",
    "            # follow a number [or plural indicator like all or many],\n",
    "            # or if it occurs at the end of the sentence.\n",
    "            elif (re.search(r\"CD\", t_sentence[i-1][1]) and\n",
    "                  re.search(r\"SENT\", t_sentence[i+1][1])):\n",
    "                result = (\"'s\", \"N/A (plural marker)\")\n",
    "            elif (len(t_sentence)-i < len(t_sentence)-1 and\n",
    "                  re.search(r\"all|many\", t_sentence[i-2][0]) and\n",
    "                  re.search(r\"CD\", t_sentence[i-2][1])):\n",
    "                result = (\"'s\", \"N/A (plural marker)\")\n",
    "\n",
    "            # not identified\n",
    "            if result == None:\n",
    "                tags_words = \" \".join([t_sentence[i-1][1], t_sentence[i][1], t_sentence[i+1][1],\n",
    "                                t_sentence[i-1][0], t_sentence[i][0], t_sentence[i+1][0]])\n",
    "                #print(tags_words)\n",
    "                return (False, sent, tags_words)\n",
    "           \n",
    "            # collect all contractions of each sentence\n",
    "            all_conts.append(result)\n",
    "        \n",
    "        # else: contraction is end of sentence\n",
    "        elif re.search(r\"\\'s$\", t_word[0]) and i == len(t_sentence)-1:\n",
    "            # filter: contraction follows named entity (number, name)\n",
    "            if t_sentence[i-1][0] in named_entities(sent):\n",
    "                # follows number: plural marker\n",
    "                if t_sentence[i-1][1] == \"CD\":\n",
    "                    result = \"N/A (plural marker)\"\n",
    "                # follows name: possesive marker\n",
    "                else:\n",
    "                    result = \"POS\"\n",
    "            # remains unidentified\n",
    "            else:\n",
    "                hit = \" \".join([t_sentence[i-1][1], t_sentence[i][1], t_sentence[i-1][0], t_sentence[i][0]])\n",
    "                #print(hit)\n",
    "                return (False, sent, hit)\n",
    "            # collect contractions identified in this filter\n",
    "            all_conts.append(result)\n",
    "\n",
    "    #print(sent, all_conts)\n",
    "    return sent, all_conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(this_cont_result):\n",
    "    \"\"\"\n",
    "    param1: list of all contraction results\n",
    "    \"\"\"\n",
    "    sent, tag_list = this_cont_result\n",
    "    replaced = sent.replace(\"\\'s\", \"{}\", 1)\n",
    "    for i in range(len(tag_list)):\n",
    "        sent = replaced.format(tag_list[i])\n",
    "        replaced = re.sub(\"^\\'s$\", \"{}\", sent, 1)\n",
    "    return this_cont_result[0], replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_file(output_file_name, all_tagged_files):\n",
    "    \"\"\"\n",
    "    make output file\n",
    "    param1: name of output file\n",
    "    param2: list of all tagged files in sub-lists\n",
    "    output: list of unclassified contractions\n",
    "    \"\"\"    \n",
    "    # make empty file\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    with open(output_path+output_file_name, 'w', encoding='utf-8') as f:\n",
    "        pass\n",
    "    left_over = []  # for unindentified contractions\n",
    "    with open(output_path+output_file_name, 'a', encoding='utf-8') as f:  \n",
    "        for index, file in tqdm(enumerate(all_tagged_files)):\n",
    "            # indicate current file (number) in output file\n",
    "            f.write(\"<meta number=\"+str(index+1)+\">\\n\")\n",
    "            for t_sentence in file:\n",
    "                cont_result = classify_contractions(t_sentence)\n",
    "                if not cont_result[0] == False:\n",
    "                    sent, replaced = replace_contractions(cont_result)\n",
    "                    f.write(replaced+\"\\n\")\n",
    "                # unidentified contracions\n",
    "                else:\n",
    "                    left_over.append(cont_result)\n",
    "    return left_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:45,  4.99s/it]\n"
     ]
    }
   ],
   "source": [
    "left_over = make_output_file(\"OUTPUT_scripts.txt\", all_tagged_files)\n",
    "#left_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_missing_file(missing_file_name, missing_list):\n",
    "    \"\"\"\n",
    "    make file for unidentified contractions\n",
    "    param1: name of file for unidentified contractions\n",
    "    param2: list of tuples for unidentified contractions\n",
    "    output: None\n",
    "    \"\"\"\n",
    "    with open(output_path+missing_file_name, 'w', encoding='utf-8') as f:\n",
    "        for entry in missing_list:\n",
    "            f.write(entry[2]+\"\\t\"+entry[1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_missing_file(\"MISSING_scripts.txt\", left_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
